{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/USER/REPO/blob/main/crisp_dm_project/notebooks/CRISP_DM_Walmart_Sales.ipynb)\n\n# CRISP\u2011DM: Walmart Sales Forecasting\n\n> Business understanding \u2192 Data understanding \u2192 Data preparation \u2192 Modeling \u2192 Evaluation \u2192 Deployment\n\n**Dataset**: Kaggle \u2014 Walmart Sales Forecast (download in the next cell).\n\n**Deliverables**: problem statement, success criteria, data audit report, EDA report, feature plan, model card, evaluation memo, deployment plan."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "!pip -q install pandas numpy scikit-learn matplotlib seaborn xgboost lightgbm imbalanced-learn joblib gradio fastapi uvicorn kaggle",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_absolute_error, r2_score, roc_auc_score, classification_report, confusion_matrix\nimport joblib\nPath('data').mkdir(exist_ok=True)\nPath('models').mkdir(exist_ok=True)\nPath('reports').mkdir(exist_ok=True)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# --- Kaggle download (make sure you've set up your Kaggle token) ---\n    # In Colab:\n    # from google.colab import files\n    # files.upload()  # upload kaggle.json\n    # !mkdir -p ~/.kaggle\n    # !cp kaggle.json ~/.kaggle/\n    # !chmod 600 ~/.kaggle/kaggle.json\n    !kaggle datasets download -d aslanahmedov/walmart-sales-forecast -p data\n    !ls -lh data\n    # If a zip is downloaded, unzip it:\n    !python - << 'PY'\nimport zipfile, glob, os\nzips = glob.glob('data/*.zip')\nfor z in zips:\n    with zipfile.ZipFile(z) as f:\n        f.extractall('data')\n        print('unzipped', z)\nPY",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Business Understanding\n- Objective & KPIs (e.g., MAE \u2264 X% on holdout)\n- Stakeholders & constraints\n- Risks & ethics (bias, leakage)\n- Project plan & assumptions"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Data Understanding\n- Inventory datasets, schemas\n- Data quality checks (missingness, ranges, duplicates)\n- Initial EDA and hypotheses"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Load your CSV here (adjust the filename)\ndf = pd.read_csv('data/Walmart.csv') if Path('data/Walmart.csv').exists() else None\nprint('Rows/cols:', None if df is None else df.shape)\ndf.head() if df is not None else None",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Quick EDA\ndef quick_eda(df):\n    display(df.head())\n    display(df.describe(include='all').T)\n    print('Missing (%) by column:')\n    display(df.isna().mean().sort_values(ascending=False).to_frame('missing_pct'))\n\nif df is not None:\n    quick_eda(df)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Data Preparation\n- Define target & features\n- Split strategy & time\u2011series guards if needed\n- Feature engineering\n- Leakage checks & pipelines"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Example preprocessing & split (edit to match columns)\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\n\nif df is not None:\n    target = 'Weekly_Sales' if 'Weekly_Sales' in df.columns else df.columns[-1]\n    X = df.drop(columns=[target])\n    y = df[target]\n    num_cols = X.select_dtypes(include=np.number).columns.tolist()\n    cat_cols = [c for c in X.columns if c not in num_cols]\n\n    pre = ColumnTransformer([\n        ('num', 'passthrough', num_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n    ])\n\n    model = RandomForestRegressor(n_estimators=300, random_state=42)\n    pipe = Pipeline([('pre', pre), ('model', model)])\n    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n    pipe.fit(X_tr, y_tr)\n    preds = pipe.predict(X_te)\n    print('MAE:', mean_absolute_error(y_te, preds))\n    print('R^2:', r2_score(y_te, preds))\n    Path('models').mkdir(exist_ok=True)\n    joblib.dump(pipe, 'models/model.joblib')",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Modeling\n- Baselines vs advanced models\n- Hyperparameters & CV design\n- Model selection rationale\n- Model card"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Evaluation\n- Business\u2011aligned metrics\n- Backtesting (if time series)\n- Error analysis & fairness checks\n- Sign\u2011off memo"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Deployment\n- Save model, inputs schema, versioning\n- FastAPI/Gradio demo\n- Monitoring plan"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Minimal FastAPI app is in src/app.py\n# Example: from a notebook, run a quick Gradio demo (replace with real features)\nimport gradio as gr\nimport numpy as np\n\ntry:\n    m = joblib.load('models/model.joblib')\n    def predict_stub(*features):\n        X = np.array(features).reshape(1, -1)\n        return float(m.predict(X)[0])\n    demo = gr.Interface(fn=predict_stub, inputs=['number']*1, outputs='number', title='Walmart Sales Predictor (demo)')\nexcept Exception as e:\n    print('Load model later after training:', e)",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}